{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting linkedin_scraper\n",
      "  Downloading linkedin_scraper-2.11.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from linkedin_scraper) (5.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from linkedin_scraper) (2.31.0)\n",
      "Requirement already satisfied: selenium in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from linkedin_scraper) (4.17.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->linkedin_scraper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->linkedin_scraper) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->linkedin_scraper) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->linkedin_scraper) (2023.11.17)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium->linkedin_scraper) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium->linkedin_scraper) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium->linkedin_scraper) (4.9.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket~=0.9->selenium->linkedin_scraper) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium->linkedin_scraper) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium->linkedin_scraper) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\aftab hussain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->linkedin_scraper) (0.12.0)\n",
      "Downloading linkedin_scraper-2.11.2-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: linkedin_scraper\n",
      "Successfully installed linkedin_scraper-2.11.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install  linkedin_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_scraper import Person, actions\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "email = \"mail@gmail.com\"\n",
    "password = \"$$$\"\n",
    "actions.login(driver, email, password) # if email and password isnt given, it'll prompt in terminal\n",
    "person = Person(\"https://www.linkedin.com/in/isabella-lantelme-29220854/\", driver=driver)\n",
    "print(person.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting utils\n",
      "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: utils\n",
      "  Building wheel for utils (pyproject.toml): started\n",
      "  Building wheel for utils (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13934 sha256=593754640ec4954783656c550945ecf28a4dc702e6f61954f2d988fa272dc0ba\n",
      "  Stored in directory: c:\\users\\aftab hussain\\appdata\\local\\pip\\cache\\wheels\\b6\\a1\\81\\1036477786ae0e17b522f6f5a838f9bc4288d1016fc5d0e1ec\n",
      "Successfully built utils\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from utils import init_driver, get_profile_urls, login,\\\n",
    "    print_scraped_data, load_config,\\\n",
    "    get_unseen_urls, connect_mongo\n",
    "from time import sleep\n",
    "from classes.UserScraper import UserScraper\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=(\"Scrape linkedin profiles based on the \" +\n",
    "                 \"queries specified in the conf file\")\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-c', '--conf',\n",
    "    type=str,\n",
    "    metavar='',\n",
    "    required=True,\n",
    "    help='Specify the path of the configuration file'\n",
    ")\n",
    "args = parser.parse_args()\n",
    "conf = load_config(args.conf)\n",
    "parameters = conf[\"parameters\"]\n",
    "credentials = conf[\"credentials\"]\n",
    "CHROME_PATH = parameters[\"CHROME_PATH\"]\n",
    "CHROMEDRIVER_PATH = parameters[\"CHROMEDRIVER_PATH\"]\n",
    "QUERIES = parameters[\"USER_QUERIES\"]\n",
    "N_PAGES = parameters[\"N_PAGES\"]\n",
    "LINUSERNAME = credentials[\"LINUSERNAME\"]\n",
    "LINPWD = credentials[\"LINPWD\"]\n",
    "MONGOUSER = credentials[\"MONGOUSER\"]\n",
    "MONGOPWD = credentials[\"MONGOPWD\"]\n",
    "HOST = parameters[\"HOST\"]\n",
    "client = connect_mongo(HOST, MONGOUSER, MONGOPWD)\n",
    "db = client[\"linkedin\"]\n",
    "users = db[\"users\"]\n",
    "driver = init_driver(CHROME_PATH, CHROMEDRIVER_PATH)\n",
    "driver.get(\"https://www.linkedin.com\")\n",
    "login(driver, LINUSERNAME, LINPWD)\n",
    "us = UserScraper(driver)\n",
    "for query in QUERIES:\n",
    "    driver.get(\"https://www.google.com\")\n",
    "    sleep(2)\n",
    "    search_query = driver.find_element_by_name('q')\n",
    "    try:\n",
    "        search_query.send_keys(query)\n",
    "    except ElementNotInteractableException:\n",
    "        print(\"ERROR :: Cannot send query. Google might be blocking\")\n",
    "        sys.exit(1)\n",
    "    sleep(0.5)\n",
    "    search_query.send_keys(Keys.RETURN)\n",
    "    profile_urls = get_profile_urls(driver, N_PAGES)\n",
    "    if len(profile_urls) == 0:\n",
    "        print()\n",
    "        print(\"WARNING :: \" +\n",
    "              \"Could not get any URLs for the query\\n\" + query)\n",
    "        print(\"Please double-check that Google is not \" +\n",
    "              \"blocking the query\")\n",
    "        continue\n",
    "    unseen_urls = get_unseen_urls(users, profile_urls)\n",
    "    if len(unseen_urls) != 0:\n",
    "        print(\"INFO :: Resuming from URL\", unseen_urls[0])\n",
    "    else:\n",
    "        print(\"INFO :: All URLs from \" + str(N_PAGES) +\n",
    "              \" Google-search page(s) for the query \" + query +\n",
    "              \" have already been scraped. \" +\n",
    "              \"Moving onto the next query if any.\")\n",
    "        continue\n",
    "    for url in unseen_urls:\n",
    "        user_data = us.scrape_user(query, url)\n",
    "        if user_data and\\\n",
    "           not db[\"users\"].count_documents(user_data, limit=1):\n",
    "            print_scraped_data(user_data)\n",
    "            users.insert_one(user_data)\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
